## Intro
- 가장 일반적이고 중요한 저장장치 ⇒ **HDD와 NVM 장치**
- 많은 유형의 대용량 저장장치가 있으므로, 모든 유형에 해당하는 내용일 경우
    - → **NVS(Nonvolatile Storage)** 용어를 사용하여 설명하도록 하겠다.

## 11.1 대용량 저장장치 구조의 개관

- 최신 컴퓨터를 위한 대용량 보조저장장치는 **하드 디스크 드라이브(HDD)와 비휘발성 메모리(NVM)** 장치에 의해 제공된다.

### 11.1.1 하드 디스크 드라이브

![image](https://user-images.githubusercontent.com/87460638/236809115-281ac966-c05b-413e-9c45-7bd1e4ff70ca.png)

> 개념적으로 HDD는 비교적 단순하다.
> 
- 각 디스크의 **플래터(platter)**는 CD처럼 생긴 원형 평판 모양이다.
    - 정보를 플래터상에 자기적으로 기록하여 저장하고 플래서틔 자기 패턴을 감지하여 정보를 읽는다.
- 읽기-쓰기 헤드는 모든 플래터의 각 표면 바로 위에서 움직인다.
    - 헤드는 모든 헤드를 한꺼번에 이동시키는 **디스크 암(disk arm)**에 부착되어 있다.
- 플래터의 표면은 원형인 **트랙(track)**으로 논리적으로 나누어져 있고,
    - 이것은 다시 **섹터(sector)**로 나누어져 있다.
    - 섹터의 2010년까지는 512바이트였다가, 이후에는 4KB 섹터로 변경되었다.
- 동일한 암 위치에 있는 트랙의 집합은 하나의 **실린더(cylinder)**를 형성한다.

- 하나의 디스크 드라이브에는 수 천개의 동심원 실린더가 존재할 수 있고, 각 트랙은 수백 개의 섹터를 포함할 수 있다.

![image](https://user-images.githubusercontent.com/87460638/236809161-bf64b103-dc42-4088-a9ca-46f418b2ec24.png)

- 디스크 드라이브 모터는 고속으로 회전한다.
    - 대부분의 드라이브는 **분당 회전수(RPM)** 단위로 표현되며, 초당 60~250회 회전한다.
    - 일부 드라이브는 사용하지 않을 때 전원이 꺼지고, I/O 요청을 받으면 회전한다.

- 회전 속도는 전송 속도와 관련있다.
    - 전송 속도는 드라이브와 컴퓨터 간의 데이터 흐름의 속도이다.
- 회전 속도는 또한 **위치 지정 시간(=** **임의 액세스 시간)**과 관련있고, 두 부분으로 구성된다.
    - 디스크 암을 원하는 실린더로 이동하는 필요한 시간 = **탐색 시간**
    - 원하는 섹터가 디스크 헤드 위치까지 회전하는데 걸리는 시간 = **회전 지연 시간**

- 일반적으로 디스크는 초당 수백~수백 메가바이트의 데이터를 전송할 수 있으며, 수 밀리초 정도의 탐색시간과 회전 지연 시간을 가진다.
    - 드라이브 컨트롤러에 DRAM 버퍼를 사용하면 성능을 향상시킬 수 있다.

- 디스크 플래터는 얇은 보호 층으로 코팅되어 있지만, 헤드는 때때로 자기 표면을 손상시킨다.
    - 이 사고를 **헤드 충돌**이라고 한다.
    - 헤드 충돌 발생 시 일반적으로 수리는 불가능하고, 전체 디스크를 교체해야 한다.

- HDD를 유지하고 있는 일부 섀시는 시스템 또는 저장장치 섀시를 종료하지 않고도 제거할 수 있다.
    - CD, DVD 및 Blu-ray 디스크를 포함한 다른 유형의 저장매체도 제거할 수 있다.

<aside>
💡 **디스크 전송률** 
디스크에 대해 공표된 성능 수치는 실제 성능 수치와 동일하지 않다. 
ex) 명시된 전송률은 항상 실질 전송률보다 높다.

</aside>

### 11.1.2 비휘발성 메모리 장치

- 비휘발성 메모리(NVM) 장치는 기계식이 아니라 전기식이다.
    - 이러한 장치는 컨트롤러 및 데이터를 저장하는 사용되는 플래시 NAND 다이 반도체 칩으로 구성된다.

**11.1.2.1 비휘발성 메모리 장치 개요**

- 플래시 메모리 기반 NVM은 디스크 드라이브와 유사한 컨테이너에서 자주 사용된다.
    
    ![image](https://user-images.githubusercontent.com/87460638/236809185-b91e05f1-669f-4e0b-bb21-6e0e0642e850.png)
    
    - 이러한 경우를 **SSD (Solid-State Disk)**라고 한다.
    - 다른 경우에는 **USB 드라이브** 또는 DRAM 스틱의 형태를 취한다.

> NVM 장치의 장점
> 
- NVM 장치는 **움직이는 부품이 없다 !**
    - ⇒ ***HDD 보다 안정성이 높으며**, **탐색시간이나 회전 지연 시간이 없으므로 더 빠를 수 있다.***
- 전력 소비량도 적다.

> NVM 장치의 단점
> 
- 기존 하드 디스크보다 메가바이트당 가격이 비싸고, 더 큰 하드 디스크보다 용량이 적다.
    - 시간이 지나면서 이러한 ***문제가 해결되고 있는 추세***이다.
    - 그러나 HDD보다 빠를 수 있게 되면서 표준 버스 인터페이스가 처리량을 크게 제한할 수 있다.

- 이러한 제한으로 인해 여러가지 개선 알고리즘이 고안되었지만,
    - 다행히 일반적으로 NVM 장치 컨트롤러에서 구현되며, 운영체제가 신경 쓰지 않아도 된다.
    - 그러나 NVM 장치는 작동 알고리즘에 따라 성능 차이가 있으므로 컨트롤러의 기능에 대한 간단한 설명이 필요하다.

**11.1.2.2 NAND 플래시 컨트롤러 알고리즘**

- NAND NVM 수명은 연 단위가 아니라 **DWPD (Drive Writes Per Day)** 로 측정된다.
- NAND 반도체는 한번 쓴 후에는 덮어쓸 수 없으므로, 일반적으로 유효하지 않은 데이터가 포함된 페이지가 있다.
    
    ![image](https://user-images.githubusercontent.com/87460638/236809227-8cd91e4c-8948-4515-9fb4-3a9f4b8f51ae.png)
    
    - 유효한 페이지를 포함하는 논리 블록을 추적하기 위해 컨트롤러는 **플래시 변환 계층 (FTL)**을 유지한다.
    - 이 테이블은 현재 유효한 논리 블록을 포함하고 있기 때문에 삭제할 수 있는 블록을 추적한다.

- 보류 중인 쓰기 요청이 있는 전체 SSD을 고려해 보자
    - SSD가 가득 차서 모든 페이지가 기록되었지만, 유효하지 않은 데이터만 포함하고 있는 블록이 있다면 → 쓰기는 지우기가 발생할 때까지 기다린 다음 수행할 수 있다.
    - 사용 가능한 블록이 없다면 → 개별 페이지에 유효하지 않은 데이터가 있으면 여전히 사용 가능한 공간이 있을 수 있다.
    - ⇒ 이 경우 **가비지 수집**이 발생할 수 있다.

> 가비지 수집은 유효한 데이터를 어디에 저장할까 ?
> 
> - → 이 문제 해결 및 성능 향상을 위해 NVM 장치는 **과잉 공급**을 사용한다.

- 가비지 수집 또는 이전의 데이터를 무효화시키는 쓰기 연산에 의해 무효화된 블록은
    - 장치가 꽉 찼다 ! → 블록 삭제되고 과잉 공급 공간에 배치된다.
    - 그렇지 않다 → 블록 삭제되고 가용 풀로 되돌아간다.

- 과잉 공급 공간은 **마모 평준화(wear leveling)**에도 도움이 된다.
    - 컨트롤러는 다양한 알고리즘을 사용하여 삭제 횟수가 적은 블록에 데이터를 배치해
    - 더 자주 삭제된 블록이 아닌 이 블록에서 후속 삭제가 일어나도록 하여 전체 마모의 수준을 평준화 시킨다.

- HDD와 마찬가지로 데이터 보호 측면에서 NVM 장치는 오류수정 코드를 제공한다.
    - 이 코드는 기록될 때 계산되어 데이터와 함께 저장되고 읽을 때 데이터와 함께 읽어 오류를 감지하고 가능한 경우 오류를 수정한다.

### 11.1.3 휘발성 메모리

- **RAM 드라이브**는 보조저장장치처럼 작동하지만
    - 시스템 DRAM의 한 영역을 할당하여 나머지 시스템에 제공하는 장치 드라이버에 의해 생성된다.

- 컴퓨터에는 버퍼링과 캐싱을 하는데, 임시 데이터 저장장치로 DRAM을 사용하는 목적은 ?
    - DRAM은 휘발성이며 RAM 드라이브를 사용하면 데이터는 시스템 크래시, 종료 또는 전원이 꺼진 후에는 지속되지 않는다.
    - 따라서 캐시와 버퍼는 프로그래머나 운영체제에 의해 할당되는 반면, RAM 드라이브를 사용하면 사용자와 프로그래머가 표준 파일 연산을 사용하여 데이터를 메모리에 임시로 보관할 수 있다.

- RAM 드라이브는 고속 임시 저장 공간으로 유용하다.
    - RAM 드라이브에 대한 I/O 작업은 파일과 내용을 생성, 읽기, 쓰기 및 삭제하는 가장 빠른 방법이다.

### 11.1.4 보조저장장치 연결 방법

- 보조저장장치는 시스템 버스 또는 I/O 버스에 의해 컴퓨터에 연결된다.
    - **ATA(Advanced Technology Attachment)**
    - **SATA(Serial ATA)**
    - **eSATA**
    - **SAS(Serial Attached SCSI)**
    - **USB(Universal Serial Bus)**
    - **FC(Fibre channel)**
    - ⇒ 이 중 가장 일반적인 연결 방법은 *SATA* 이다.

- NVM 장치는 HDD보다 훨씬 빠르기 때문에 산업계에서는 **NVMe(NVM express)** 라는 NVM 장치를 위한 특별하고 빠른 인터페이스를 만들었다.
    - NVMe는 장치를 시스템 PCI 버스에 직접 연결하여 다른 연결 방법과 비교해 처리량을 높이고 지연시간을 줄인다.
    
- 버스에서의 데이터 전송은 **컨트롤러**(or **호스트 버스 어댑터(HBA)**)라고 하는 특수 전자 프로세서에 의해 수행된다.
    - **호스트 컨트롤러**는 버스의 컴퓨터 쪽에 있는 컨트롤러이다.
    - 각 저장장장치에는 **장치 컨트롤러**가 내장되어 있다.

- **드라이브에서의 데이터 전송**은 **캐시와 저장 매체 사이**에서 발생하며
    - **호스트로의 데이터 전송**은 DMA를 통해 **캐시와 호스트 DRAM 사이**에서 빠른 전자 속도로 발생한다.

### 11.1.5 주소 매핑

- 저장장치는 논리 블록의 커다란 1차원 배열처럼 주소가 매겨진다.
    - **논리 블록**은 가장 작은 전송 단위이다.
    - 각 논리 블록은 물리 섹터 또는 반도체 페이지로 매핑된다.
    - 논리 블록의 주소(**LBA**)

- 논리 블록의 1차원 배열은 장치의 섹터들 또는 페이지들에 매핑된다.
    - 매핑은 해당 트랙을 순서대로 완료한 후에 해당 실린더의 나머지 트랙을 매핑한 다음 나머지 실린더를 바깥쪽에서 안쪽 순으로 매핑한다.

- HDD에서 주소 매핑을 사용하면 적어도 ***이론적으로는*** 논리 블록 번호를 실린더 번호, 해당 실린더 내의 트랙 번호, 해당 트랙 내의 섹터 번호로 구성된 구식 디스크 주소로 변환할 수 있다.
    - 그러나 ***현실에선 3가지 이유로 인해 변환 수행이 어렵다 !***
    1. 대부분의 드라이브에는 결함이 있는 섹터가 있지만 매핑은 드라이브의 다른 곳에 있는 예비 섹터로 대체하며 이를 숨긴다.
    2. **트랙당 섹터 수는 일부 드라이브에서 일정하지 않다.**
    3. 디스크 제조업체는 LBA와 물리 주소 간의 매핑을 내부적으로 관리하므로, 현재 드라이브에서는 LBA와 물리적 섹터 간에 관계가 거의 없다.

> 위에서 2번째 이유를 좀 더 살펴보자
> 
- **고정 선형 속도(CLV, Constant Linear velocity)** 를 사용하는 장치에서는 트랙당 비트의 밀도가 일정하다.
    - 트랙은 디스크 중심으로부터 멀어질수록 길이가 길어져서 더 많은 섹터를 가질 수 있다.
    - 따라서 ***현대의 디스크는 실린더들을 구역으로 나눈다.***

- 트랙당 섹터의 수는 한 구역 안에서는 일정하지만 구역 간에는 차이가 난다.
    - 안쪽 구역으로부터 바깥으로 감에 따라 트랙당 섹터 수는 증가한다.
- 드라이브는 헤드가 바깥쪽에서 안쪽 트랙으로 감에 따라 회전 속도가 늘어난다.
    - 헤드 아래를 통과하는 데이터의 비율을 동일하게 하기 위함
    - ⇒ **고정각 속도(CAV, Constant Angular Velocity)**

## 11.2 디스크 스케줄링

- 장치 **대역폭**은 **전송된 총 바이트 수 ÷ (첫 번째 서비스 요청 +.. + 마지막 전송 완료까지의 시간)**의 값이다.
    - 저장장치 I/O 요청이 처리되는 순서를 관리하여 접근 시간과 대역폭을 모두 향상할 수 있다.

- 프로세스가 입출력을 해야할 때마다 운영체제에 시스템 콜을 한다.
    - 이 호출에는 여러가지 인수가 주어진다.
        - 이 작업이 입력 또는 출력인지의 여부
        - 연산이 수행될 파일을 가리키는 열린 파일 핸들
        - 전송을 위한 메모리 주소
        - 정송할 데이터의 양

- 헤드 탐색을 피해 성능을 최적화할 수 있는 장치에 대한 요청 큐를 유지하면
    - → 장치 드라이버는 큐의 순서를 조정하여 성능을 향상시킬 기회를 갖게 된다.

- 과거 HDD 인터페이스에서는 호스트가 사용할 수 있는 트랙과 헤드를 지정해야 했다.
    - 때문에 디스크 스케줄링 알고리즘에 큰 노력을 기울였다.
- 2000년 이후의 새로운 드라이브는 이러한 컨트롤을 호스트에 노출하지 않고, LBA를 물리 주소로 매핑하는 책임이 드라이브에 있게 했다.
    - 디스크 스케줄링의 현재 목표는 공정성, 적시성, 최적화에 있다.
    - 이를 실현시키기 위해선 여전히 디스크 스케줄링 알고리즘이 필요하다.

### 11.2.1 선입 선처리 스케줄링

- 디스크 스케줄링의 가장 간단한 형태
- **선입 선처리 스케줄링(FCFS Scheduling)** 은 본질적으론 공평해 보이지만 빠른 서비스를 제공하지는 못한다.

<aside>
💡 ex) 다음과 같은 입출력 요청이 디스크 큐에 왔다고 생각해 보자.

![image](https://user-images.githubusercontent.com/87460638/236809283-2e819e05-3b68-4220-bdd3-680482b0d525.png)

만약 디스크 헤드가 현재 실린더 53에 있으면 → 헤드는 먼저 53에서 98로 이동

그다음엔 183, 37, 122 … 그리고 67로 이동하여 모두 640 실린더를 헤드가 가로지르게 된다.

![image](https://user-images.githubusercontent.com/87460638/236809314-71e96961-949f-4fc9-afe9-f1c64d4424ae.png)

</aside>

- 이 스케줄링의 문제점은 갔던 곳을 반복해서 감으로써 이동 거리가 길어진다.
    - ⇒ 따라서 ***근처에 있는 헤드는 함께 서비스해 준다면 헤드의 총 이동 거리를 많이 줄일 수 있고 성능도 향상할 수 있다.***

### 11.2.2 SCAN 스케줄링

- **SCAN 알고리즘**에서는 디스크 암이 디스크의 한끝에서 시작하여 다른 끝으로 이동하며, 가는 길에 있는 요청을 모두 처리한다.
    - 다른 한쪽 끝에 도닥하면 역방향으로 이동하면서 오는 길에 있는 요청은 모두 처리한다.
    - 스케줄링 동작이 엘레베이터와 유사하다 하여 **엘레베이터 알고리즘**이라고도 부른다.

<aside>
💡 앞서 FCFS 스케줄링의 예를 SCAN 스케줄링 방식으로 풀어보자.

디스크 암의 현재 위치가 53이며 0방향으로 이동하고 있다고 가정한다면, 가장 먼저 37과 14를 처리한다. 이후 역방향으로 이동하면서 65, 67, 68, … 순으로 처리할 것이다.

![image](https://user-images.githubusercontent.com/87460638/236809348-4299ebcd-5729-451b-ba35-9ef607609f04.png)

</aside>

- 이 스케줄링의 문제점은 헤드가 한쪽 끝에 도달했을 때 헤드 바로 근처에는 대기 중인 요청이 거의 없고, 반대쪽 끝으로 가장 많은 요청이 몰려 있을 것이다.
    - ⇒ 따라서 되돌아서 현재 있는 위치부터 서비스를 시작하기보다 ***가장 반대쪽으로부터 서비스를 시작해보자.***

### 11.2.3 C-SCAN 스케줄링

- **C-SCAN(Circle-SCAN) 스케줄링**은 각 요청에 걸리는 시간을 좀 더 균등하게 하기 위한 SCAN의 변형이다.
    - SCAN과 같이 한쪽으로 헤드를 이동해 가면서 요청을 처리하지만, 한쪽 끝에 다다르면 처음 시작했던 자리로 다시 돌아가서 서비스를 시작한다.

<aside>
💡 앞서 예제에 C-SCAN을 적용해보자.
디스크 암이 0에서 199로 이동할 때 요청이 스케줄 되고 초기 헤드 위치가 다시 53이라고 가정하면 다음과 같이 서비스된다.

![image](https://user-images.githubusercontent.com/87460638/236809392-2082d180-b1a7-499a-bb07-bd380e879add.png)

</aside>

- 이 스케줄링은 기본적으로 실린더를 최종 실린더에서 첫 번째 실린더로 되돌아가는 ***순환 리스트로 처리***한다.

### 11.2.4 디스크 스케줄링 알고리즘의 선택

- SCAN 및 C-SCAN은 기아 문제를 일으킬 가능성이 작기 때문에 디스크에 많은 부하를 주는 시스템의 성능을 향상한다.
    - 그래도 기아가 계속 될 수 있기 때문에 → **마감 시간(deadline) 스케줄러**가 생겨났다.
    - 마감 시간 스케줄러는 읽기와 쓰기별로 큐를 유지 관리하며 프로세스가 쓸 때보다 읽을 때 봉쇄될 가능성이 높으므로 읽기 연산에 높은 우선순위를 부여한다.

- 마감 시간 I/O 스케줄러는 Linux RedHat7 배포에서는 디폴트였지만, **RHEL** 7에는 다른 두 가지가 포함되어 있다.
    - **NOOP 스케줄러**는 NVM 장치와 같은 빠른 저장장치를 사용하는 CPU-중심 시스템에 선호된다.
    - **CFQ(Completely Fair Queuing) 스케줄러**는 SATA 드라이브의 디폴트 스케줄러이다.

- CFQ는 실시간, 디폴트 및 유휴의 세가지 큐를 유지한다.
    - 각각은 언급된 순서대로 다른 큐에 대해 배타적 우선순위가 부여되고 기아 현상이 발생할 수 있다.
    - 프로세스가 더 많은 I/O 요청을 곧 내릴 것을 이전 기록을 기반으로 예측한다.
    - 그렇게 결정되면 대기 중인 다른 요청을 무시하고 새 I/O를 기다린다.

## 11.3 NVM 스케줄링

- I/O는 순차적으로 또는 무작위로 발생할 수 있다.
    - 읽거나 쓸 데이터가 읽기/쓰기 헤드 근처에 있기 때문에 HDD 및 테이프와 같은 기계 장치에는 순차적 액세스가 최적이다.
- **초당 입/출력 연산 수(IOPS)**로 측정되는 무작위 액세스 I/O는 HDD 디스크 헤드 이동을 유발한다.
    - 무작위 액세스 I/O는 NVM에서 훨씬 빠르다.

- HDD 헤드 탐색을 최소화하고 미디어에 대한 데이터 읽기 및 쓰기가 강조되는 raw 순차 처리량 측면에서는 NVM이 더 적은 이득을 제공한다.
    - 읽기의 경우 두 가지 유형의 장치 성능은 동등하거나 NVM이 10배 정도 이득을 얻는다.
    - 쓰기 성능은 장치 수명 동안 일관되지만 NVM 장치의 쓰기 성능은 장치가 얼마나 찼는지와 얼마나 마모되었는지에 따라 달라진다.

- NVM 장치의 수명과 성능을 향상하는 한 가지 방법은 ***파일이 삭제될 때 파일 시스템이 장치에 알리도록 하여 장치가 해당 파일이 저장된 블록을 지울 수 있도록 하는 것이다.***
    
    ~~… 14.5.6절에서 자세히 설명함~~
    

- 가비지 수집이 성능에 미치는 영향에 대해 알아보자.
    - 하나의 쓰기 요청으로 인해 페이지 쓰기(데이터), 하나 이상의 페이지 읽기(가비지 수집에 의해) 및 하나 이상의 페이지 쓰기(가비지 수집된 블록의 유효한 데이터)가 발생한다.
- 응용 프로그램이 아니라 가비지 수집 및 공간 관리를 수행하는 NVM 장치에 의한 I/O 요청 생성을 **쓰기 증폭**이라고 하며, 장치의 쓰기 성능에 큰 영향을 줄 수 있다.

## 11.4 오류 감지 및 수정

- **오류 감지**는 문제가 발생했는지 여부를 결정한다.
    - 문제를 감지하면 시스템은 오류가 전파되기 전에 작업을 중단하거나 사용자 또는 관리자에게 오류를 보고하거나 실패하기 시작했거나 이미 실패한 장치에 경고할 수 있다.

- 모든 단일 비트 오류는 메모리 시스템에 의해 감지된다.
    - 그러나 이중 비트 오류는 감지되지 않을 수 있다.
    - ⇒ **패리티**는 비트의 XOR을 수행하여 쉽게 계산한다.
        - 모든 바이트의 메모리에 대해 패리티를 저장하기 위한 여분의 메모리가 필요하다.

- 패리티는 고정 길이 워드의 값을 계산, 저장 및 비교하기 위해 나머지 연산을 수행하는 **체크섬**의 형태이다.
- 네트워킹에서 일반적으로 사용되는 또 다른 오류 감지 방법은 해시 함수를 사용하여 다중 비트 오류를 감지하는 **순환 중복 검사(CRC, Cylic Redundancy Check)**이다.

- **오류 수정 코드(ECC, Error-Correction Code)** 는 문제를 감지할 뿐만 아니라 보정한다.
    - 보정은 알고리즘과 저장장치를 추가로 사용하여 수행한다.
    - 코드는 필요한 추가 저장장치의 양과 보정할 수 있는 오류 개수에 따라 다르다.

- ECC는 몇 비트의 데이터만 손상된 경우 컨트롤러가 변경된 비트를 식별하고 올바른 값을 계산할 수 있도록 충분한 정보를 포함한다.
    - 그런 다음 복구 가능한 **소프트 오류**를 보고한다.
    - 너무 많은 변경이 발생하고 ECC가 오류를 수정할 수 없는 경우 수정할 수 없는 **하드 오류**가 표시된다.
    - 컨트롤러는 섹터 또는 페이지를 읽거나 쓸 때마다 ECC 처리를 자동으로 수행한다.

## 11.5 저장장치 관리

### 11.5.1 드라이브 포매팅, 파티션, 볼륨

- 새로운 저장장치는 아무런 정보도 없는 비어있는 판 또는 초기화되지 않은 반도체 저장셀의 집합이다.
    - 저장장치는 자료를 저장하기 전에 컨트롤러가 읽고 쓸 수 있도록 섹터들로 나눠져 있어야 한다.
    - NVM 페이지는 초기화되어야 하고, FTL이 생성되어야 한다.
    - 이러한 과정을 **저수준 포매팅(low-level formatting)** 또는 **물리적 포매팅**이라고 한다.

- 저수준 포매팅은 각 저장장치 위치마다 특별한 자료구조로 장치를 채운다.
    - 섹터 또는 페이지를 위한 자료구조는 보통 헤더, 자료구역과 트레일러로 구성된다.

- 대부분의 하드 디스크는 저수준 포매팅이 되어 생산된다.
    - 이렇게 세팅되어야 하드 디스크를 제조하면서 디스크를 제조하면서 디스크 품질을 검사할 수 있고,
    - 논리적 블록 주소를 손상되지 않은 섹터 또는 페이지들에게 할당해 줄 수도 있게 된다.

> 드라이브를 사용하여 파일을 보유하려면 운영체제가 자체 데이터 구조를 장치에 기록해야 한다.
> 
> - 이 작업을 세 단계로 수행한다.
1. 장치를 하나 이상의 블록 또는 페이지 그룹으로 **파티션**하는 것이다.
    1. 운영체제는 각 파티션을 별도의 장치인 것처럼 취급할 수 있다.
    2. 파티션 정보는 저장장치의 고정된 위치에 고정된 형식으로 기록된다.
    3. 운영체제에서 장치를 인식하면 타피션 정보를 읽은 다음, 파티션에 해당하는 장치 항목을 생성한다.
    4. 운영체제에 파일 시스템을 포함하는 각 파티션을 지정된 위치에 마운트하고 함께 읽기 전용과 같은 마운트 옵션을 사용하도록 지시한다.
    5. 파일 시스템을 **마운트**하면 시스템과 해당 사용자가 그 파일 시스템을 사용할 수 있게 된다.
2. **볼륨** 생성 및 관리이다.
    1. 이 단계는 암시적으로 적용된다. → 해당 볼륨을 마운트하여 사용할 수 있다.
    2. 다른 경우, 볼륨 생성 및 관리가 명시적으로 행해진다.
3. **논리적 포매팅** 또는 파일 시스템의 생성이다.
    1. 이 단계는 운영체제는 초기 파일 시스템 자료구조를 장치에 저장한다.
    2. 이러한 자료구조에는 가용 공간과 할당된 공간의 맵과 초기의 빈 디렉터리 등이 포함된다.

- 효율성을 높이기 위해 대부분의 파일 시스템은 블록을 종종 **클러스터**로 묶는다.
    - 효과적으로 I/O는 순차 접근을 더 많이 하고 랜덤 액세스 특성을 줄이는 것을 보장한다.

- 일부 운영체제는 특정 프로그램이 파일 시스템 자료구조 없이도 파티션을 논리 블록의 대용량 순차 배열처럼 사용할 수 있게 한다.
    - 이 배열을 **raw 디스크** 라고 한다.
    - 이 배열에 대한 I/O를 **raw I/O** 라고 한다.

### 11.5.2 부트 블록

- 컴퓨터가 실행을 시작하려면 실행할 초기 프로그램이 존재해야 한다.
    - 이 초기 **부트스트랩**은 시스템 마더보드의 NVM 플래시 메모리 펌웨어에 저장되며 알려진 메모리 위치에 매핑된다.
    - 부트 파티션이 있는 장치를 **부트 리스크** 또는 **시스템 디스크** 라고 한다.

- 부트스트랩 NVM의 코드는 저장장치 컨트롤러에 부트스트랩 프로그램을 메모리에 올리도록 지시하고, 그 프로그램의 수행을 시작한다.
    - 부트스트랩 프로그램 본체는 부트스트랩 로더보다 복잡하며 그 임무도 저장장치 내 임의의 장소에 저장된 운영체제는 커널을 찾아내고 그것을 시작시키는 일까지 하게 된다.

> Windows에서의 부트 프로세스
> 

![image](https://user-images.githubusercontent.com/87460638/236809452-4422be67-07da-43c3-9711-455b637b7e85.png)

- 1개 이상의 파티션으로 드라이브를 파티션할 수 있게 하고 **부트 파티션**이라고 된 한 파티션에 운영체제와 장치 드라이버들이 저장된다.
- Windows 시스템은 부트 코드를 하드 디스크의 첫 번째 논리 블록 또는 NVM 장치의 첫번째 페이지(**마스터 부트 레코드** 또는 **MBR**)에 배치한다.
- 부팅은 시스템 펌웨어에 상주하는 코드를 실행하여 시작한다.
- 시스템이 부팅 파티션을 식별하면 해당 파티션에서 첫 번째 섹터/페이지(**부트 섹터**)를 읽고, 커널로 안내한다.
- 그런 다음, 다양한 서브 시스템 및 시스템 서비스 적재를 포함하여 나머지 부트 프로세를 계속한다.

### 11.5.3 손상된 블록

- 보통은 한두 개 섹터에 결함이 생긴다.
    - 어떤 경우는 공장에서 출고될 때 이미 **손상 블록**을 가지고 나오는 수도 있다.
    - ⇒ ***손상된 블록들은 디스크와 컨트롤러에 따라 다양한 방법으로 처리될 수 있다.***

- 구형 IDE 컨트롤러를 가진 디스크는 손상된 블록들을 **수동으로 처리**한다.
    - 한가지 전략은 디스크 포맷 중에 디스크를 스캔하여 손상된 블록이 있는지 검사하는 것이다.
    - 발견된 손상된 블록은 사용 불가라고 표시하여 파일 시스템이 그 블록을 할당하지 않도록 한다.

- 정교한 디스크는 손상된 디스크를 현명하게 처리한다.
    - 컨트롤러는 **손상 블록의 디스크를 유지**한다.
    - 리스트는 공장에서 저수준 포맷하는 동안 초기화되고, 디스크가 사용되는 동안 계속 유지한다.
    - 저수준 포매팅은 운영체제가 볼 수 없는 **예비 섹터를 남겨 놓는다.**
    - 컨트롤러는 이러한 예비 섹터 중 하나를 **손상된 섹터와 교체**시킨다.
        - 이 기법을 **섹터 예비(sector sparing)** or **섹터 포워딩(sector forwarding)**이라고 한다.

- 일반적으로 손상된 섹터는 아래와 같이 처리된다.
    - 운영체제가 논리 블록 87을 읽으려고 시도한다.
    - 컨트롤러가 ECC를 계산한 결과 그 섹터가 손상된 것을 알게 된다.
        - 이 사실을 입출력 오류로 운영체제에 보고한다.
    - 장치 컨트롤러가 손상된 섹터를 예비 섹터로 교체한다.
    - 그 후에는 논리적 블록 87이 요청될 때마다 변경된 새로운 섹터 주소로 가게 된다.

- 이처럼 ***컨트롤러가 재배치를 처리하면 디스크 스케줄링 알고리즘에 의한 최적화가 무산될 수 있음을 주의하자 !***
    - 이에 대한 해결책으로, 일부 디스크들은 포매팅할 때 예비 섹터를 실린더마다 배치하고 예비 실린더도 배치한다.
    - 또한 섹터가 손상되면 가능한 동일한 실린더에서 예비 섹터를 찾아 대체한다.

- 예비 섹터를 관리하는 다른 방안은 **섹터 밀어내기**에 의한 손상 섹터를 처리할 수 있도록 한다.
    - 섹터 밀어내기는 결함이 생긴 섹터가 발생하면 예비 섹터를 다음 예비 섹터로 복사한다.
        - 예를 들면, 논리 블록 17번에 결함 생겼고 첫번째 예비 섹터는 202 다음에 있다고 할 때, 섹터 202는 예비 섹터로 복사되고 201은 202, 200은 201 등으로 복사된다.

- 복구 가능한 **연성 에러(soft error)**는 블록 데이터의 사본이 만들어지고 블록이 스페어 또는 슬립 되는 장치 활동을 트리거할 수 있다.
    - 그러나 복구가 안되는 **경성 에러(hard error)**는 데이터를 잃게 된다.
    - 이렇게 손상된 블록 내용은 백업으로부터 가져와야 하는 등 사람의 개입이 필요하다.

## 11.6 스왑 공간 관리

- **스왑 공간 관리**는 운영체제가 수행해야 하는 또 다른 하위 수준의 작업이다.
    - 드라이브는 메모리보다 훨씬 더 느리다.
    
    ⇒ 따라서 스왑 공간은 가상 메모리 성능에 커다란 영향을 미치고 스왑 공간의 설계와 구현은 매우 중요하다.
    

### 11.6.1 스왑 공간 사용

> 사용하는 메모리 관리 알고리즘에 따라 스왑 공간은 다양한 방법으로 운영된다.
> 
- 스와핑을 사용하는 시스템은
    - 코드와 데이터 전체, 즉 프로세스 이미지 전체를 스왑 공간에 유지하게 할 수 있다.
- 페이징을 사용하는 시스템은
    - 단순히 메인 메모리에서 밀려나는 페이지들을 이 공간에 저장할 수 있다.

⇒ 따라서 스왑 공간의 크기는 물리 메모리 크기, 가상 메모리 크기, 가상 메모리가 사용되는 방식 등에 따라 적게는 수 MB에서 많게는 수 GB까지 있다.

- 스왑 공간은 **예상보다 크게 잡는 것이 안전하다.**
    - **왜?** 시스템을 운영하다가 스왑 공간이 바닥나면 프로세스들을 도중에 중단시켜야 하고, 최악의 상황에는 ***시스템 전체가 crash 될 수 있기 때문***이다.

### 11.6.2 스왑 공간 위치

- 스왑 공간은 두 군데에 있을 수 있다.
    - 일반 파일 시스템이 차지하고 있는 **공간 안에서** 만들어 질 수 있거나
    - **별도의 파티션**을 만들어 사용할 수 있다.

- 스왑 공간은 별도의 **raw 파티션**에 만들어질 수 있다.
    - 일반 파일이나 디렉터리는 이 공간에 저장되지 않는다.
    - 이 파티션은 별도의 **스왑 관리 루틴에 의해 스와핑을 하는 데에만 사용**된다.

- 스왑 관리 루틴은 공간 효율성보다 **속도 효율성을 최적화하기 위한 알고리즘을 사용**한다.
    - **왜?** ***스왑공간은 사용될 때 파일 시스템보다 훨씬 자주 접근***되기 때문이다.

- 일부 운영체제들은 유연하게 별개의 파티션과 파일 시스템 공간 모두에 스왑 공간이 가능하다.
    - ex) 리눅스

### 11.6.3 스왑 공간 관리: 예

- Unix는 하드웨어 페이징 기술이 발달함에 따라 페이징과 스와핑을 함께 사용하는 방식으로 발전됐다.

- Solaris 1은 개발자들이 효율을 높이고 기술 발전을 반영하도록 표준 Unix 메소드들을 변경했다.
    - 페이지 교체 시에 선택된 페이지는 스왑 공간으로 스왑아웃 시키기보다 그냥 덮어 버린다.
    - 그 페이지 다시 필요하게 되면 파일 시스템에서 한 번 더 읽는 방식으로 진행한다.
- Solaris의 다음 버전은 한 페이지가 물리 메모리에서 쫓겨날 때 스왑공간을 할당하는 것이다.
    - 이 기법은 페이징이 적은 현대 컴퓨터들에게 더욱 좋은 성능을 보인다.

- Linux는 스왑 공간을 익명 메모리에만 사용한다는 점에서 Solaris와 유사하다.
    - 각 스왑 영역은 연속된 4KB의 **페이지 슬롯(page slot)**들로 구성되고,
    - 이 페이지 슬롯들에 스왑된 페이지들이 적재된다.
- 각 스왑 영역은 **스왑 맵(swap map)**으로 연관된다.
    - 스왑 맵은 정수 카운터들의 배열이고 각 항목은 스왑 영역에 한 페이지 슬롯에 해당한다.

## 11.7 저장장치 연결

- 컴퓨터는
    - 호스트에 연결하는 방식
    - 네트워크로 연결된 저장장치
    - 클라우드 저장장치 등의 3가지 방법으로 보조저장 장치에 접근한다.

### 11.7.1 호스트 연결 저장장치

- **호스트 연결 저장장치**는 로컬 I/O 포트를 통해 액세스 하는 과정이다.
    - 하나 또는 다수의 SATA 포트를 가진다.

- 고성능 워크스테이션과 서버는 일반적으로 더 많은 저장장치가 필요하거나 저장장치를 공유해야 한다.
    - ⇒ **광섬유 채널(FC, Fire Channel)**과 같은 정교한 I/O 아키텍처를 사용한다.
        - FC는 광섬유 또는 4-선구리 케이블 위에서 작동할 수 있는 고속 아키텍처이다.
        - 다수의 호스트와 저장장치가 기본 망에 연결되어 I/O 통신에 큰 유통성을 제공한다.

- 다양한 저장장치가 호스트 연결 저장장치로 사용하기에 적합하다.
    - HDD; NVM(CD, DVD,Blu-ray) 및 드라이브; 및 **SAN(Storage-Area Network)**등이 있다.

### 11.7.2 네트워크 연결 저장장치

![image](https://user-images.githubusercontent.com/87460638/236809498-f8da901c-549e-448a-985d-164d3bb009cc.png)

- **NAS(Network-Attached Storage)**는 네트워크를 통해 저장장치에 대한 액세스를 제공한다.
    - NAS 장치는 특수 목적 저장장치 시스템이거나 네트워크의 다른 호스트에 저장장치를 제공하는 일반 컴퓨터 시스템일 수 있다.

- 클라이언트는 Unix 및 Linux 시스템용 NFS 또는 Windows 시스템용 CIFS와 같은 원격 프로시저 호출 인터페이스를 통해 네트워크 연결 저장장치에 액세스 한다.
    - RFC(원격 프로시저 호출)는 IP 네트워크 상에서 TCP/UDP를 통해 전달된다.
        - LAN 상에서 수행된다.

- CIFS 및 NFS는 RFC를 사용하여 NAS 에 액세스 하는 호스트 간에 파일을 공유할 수 있도록 다양한 락킹 기능을 제공한다.

- 네트워크에 부착된 저장장치는 쉬운 네이밍과 접근 기법을 공유할 수 있는 방법을 제공한다.
    - 하지만 효율적이지 못하고, 일부 직접 부착된 저장장치보다 낮은 성능을 가지고 있다.

- 최근의 **iSCSI**는 네트워크로 연결된 저장장치 프로토콜이다.
    - 이것의 핵심은 SCSI 프로토콜을 전송하기 위해 IP 네트워크를 사용한다.
    - 따라서 SCSI 케이블을들이 아닌 네트워크가 호스트들과 호스트들의 저장장치를 연결한다.
    - ⇒ 다시 말해, ***호스트들은 호스트들에 직접 연결되지 않은 저장장치들도 직접 연결된 것처럼 저장장치를 사용할 수 있다.***
    

### 11.7.3 클라우드 저장장치

- 클라우드 저장장치는 네트워크 연결 저장장치와 유사하게 네트워크를 통해 저장장치에 액세스할 수 있다.

> NAS와 클라우드 저장장치의 차이점
> 
1. 클라우드 저장장치는 NAS와 다르게 유료로 저장장치를 제공하는 원격 데이터 센터에 인터넷 또는 다른 WAN을 통해 접속하여 액세스 된다.
2. 저장장치가 접근되는 방식과 사용자에게 제공되는 방식이 다르다.
    - NAS는
        - CIFS 또는 NFS 프로토콜을 사용하면 → **다른 파일 시스템인 것처럼 접근**된다.
        - iSCSI 프로토콜을 사용하면 → **raw 블록 장치로 접근**된다.
    - 클라우드 저장장치는
        - **API 기반**이며, 프로그램은 API를 사용하여 저장장치에 접근한다.

- 기존 프로토콜 대신 API를 사용하는 이유 ? ⇒ ***WAN의 지연시간 및 장애 시나리오*** 때문 !
    - NAS 프로토콜은 LAN에서 사용하도록 설계되었고, WAN보다 시간이 짧으며 저장장치 사용자와 장치 간의 연결이 끊어질 가능성이 훨씬 작다.
    - LAN 연결이 장애를 일으키면 NFS 또는 CIFS를 사용하는 시스템은 연결이 복구될 때까지 중단될 수 있다.
    

### 11.7.4 SAN과 저장장치의 배열

![image](https://user-images.githubusercontent.com/87460638/236809527-0e179f25-ad5e-47e1-9b07-5a3d43b63f70.png)

- **SAN(Strage-Area Network)**은 서버들과 저장장치 유닛들을 연결하는 사유 네트워크이다.
    - 장점 - **유연성**
        - 여러 호스트와 저장장치가 같은 SAN에 부착될 수 있고, 저장장치는 동적으로 호스트에 할당될 수 있다.
        - 저장장치 배열은 RAID 형식으로 보호되거나 보호되지 않는 드라이브일 수 있다.
            
            ⇒ **JBOD (Just a Bunch Of Disks)**
            
        - SAN 스위치는 호스트와 저장장치 간의 액세스를 허용하거나 금지한다.
    - 단점 - 비용이 많이 든다.

![image](https://user-images.githubusercontent.com/87460638/236809606-3c4ff7be-17a3-4ce2-9539-f67cfd8afde8.png)

- 저장장치 배열은 SAN 포트, 네트워브 또는 둘 다를 포함하는 특수 목적 장치이다.
    - 또한 데이터를 저장하는 드라이브와 저장장치를 관리하고 네트워크를 통해 저장장치에 액세스할 수 있는 컨트롤러를 포함한다.
    - 배열의 기능에는 네트워크 프로토콜, UI, RAID 보호, 스냅숏 등이 포함된다.
    - 일부 저장장치 배열에는 SSD가 포함된다.

- iSCSI의 단순함 때문에 사용이 점차 늘고 있지만, FC는 가장 일반적인 SAN 내부연결 방법이다.
    - 또 다른 내부연결 구조는 **InfiniBand(IB)**라고 하는 특수 목적의 버스 아키텍처이다.
    - 이것은 서버들과 스토리지 유닛을을 연결하는 고속의 내부연결망을 지원하는 하드웨어와 소프트웨어를 장착한다.

## 11.8 RAID 구조

- 저장장치가 점차 소형화 & 저렴해지면서 현재 컴퓨터 시스템에는 다수의 드라이브를 부착하는 것이 저렴하게 적용할 수 있다.
    - 시스템이 많은 수의 드라이브를 가지고 있고, 병렬적으로 운영된다면
    → 데이터 읽기, 쓰기 비율을 향상할 수 있다.
    - 또한 중복 정보가 여러 드라이브에 저장되기 때문에
    → 데이터 저장의 신뢰성을 높일 수 있다.
    - ⇒ 정리하면, 하나의 드라이브 오류로 데이터가 분실되지 않는다.
- 따라서 **RAID(Radundant Array of Inexpensiv Disk)**라 불리는 다양한 디스크 구성 기술은 일반적으로 성능과 신뢰성 이슈를 해결하는 데 역점을 두고 있다.

- 과거에 RAID는 크고 가격이 비싼 디스크들의 값싼 대안으로 간주했다.
    - 하지만 현재는 높은 신뢰성과 데이터 전송률 때문에 사용된다.

### 11.8.1 중복으로 신뢰성 향상

- N개의 디스크로 구성된 세트에 대수의 디스크에 오류가 발생할 확률은 하나의 디스크의 오류 확률보다 훨씬 크다.
    - 따라서 신뢰성의 문제 해결방안은 **중복을 허용**하는 것이다.
    - 분실된 정보를 다시 빌드하기 위해 오류의 경우마다 사용되는 별도의 정보를 저장함으로써 
    → ***디스크가 고장나더라도 데이터가 손실되지 않는다 !***

- 중복을 도입하는 가장 간단한 접근 방법은 모든 드라이브의 복사본을 만드는 것이다.
    - ~~이렇게 되면 비용이 많이 들긴 한다.~~
    - 이 기술은 **미러링(mirroring)**이라고 불린다.
- 미머링을 사용하는 경우 하나의 논리 디스크는 두 개의 물리 드라이브로 구성되고 모든 쓰기 작업은 두 드라이브에 모두에서 실행된다.
    - 이러한 결과를 **미러드 볼륨**이라고 한다.
    - 이 볼륨 중 하나의 드라이브에 오류가 발생하면, 다른 드라이브로부터 데이터를 읽어 들인다.

- 미러드 볼륨의 MTBT는 두 가지 요소에 좌우된다.
    1. 단일 드라이브의 MTBF
    2. 손상된 드라이브를 교체하고 데이터를 다시 저장하는 데 소요되는 **평균 수리 시간**

- 두 드라이브의 고장이 독립적이라고 가정하면, 한 드라이브의 고장은 다른 드라이브의 고장과 무관하다.
    - 그러나 이 가정은 할 수 없는 가정이다 ! 
    → 즉, 드라이브 고장은 오류 확률을 증가시키기 때문에 다른 드라이브 역시 영향을 미친다.
    - 그러나 모든 고려상황에도 불구하고 ***미러링 드라이브 시스템은 싱글 시스템보다 더 높은 신뢰성을 제공한다.***

### 11.8.2 병렬성을 이용한 성능 향상

- 여러 드라이브를 사용할 경우 여러 드라이브에 걸쳐 데이터 스트라이핑을 사용하여 전송 비율을 향상할 수 있다.
    - 가장 간단한 형식으로 **데이터 스트라이핑**이 있다.

- 데이터 스트라이핑은 여러 드라이브에 각 바이트의 비트를 나누어 저장함으로써 구성된다.
    - 이러한 라이핑을 **비트 레벨 스트라이핑(bit-level striping)**이라 한다.
    - 예를 들어, 8개의 드라이브를 가지고 있다면, → 각 드라이브 i에 각 바이트의 비트 i를 쓰는 것이다.

- 비트 레벨 스트라이핑은 8의 배수 또는 8의 약수로 드라이브 개수를 일반화할 수 있다.
    - 예를 들어, 4개의 드라이브로 구성한다면 → i번째 비트와 각 바이트의 4+i비트는 i번째 드라이브로 간다.
- 스트라이핑을 반드시 비트 레벨로 할 필요는 없다.
    - 예를 들어, **블록 단위 스트라이핑**에서는 파일으리 블록은 여러 드라이브에 거쳐 스트라이핑된다.
    - n개의 드라이브에서 파일의 i번째 블록은 (i mod n) + 1 드라이브로 간다.
    - 블랙 단위 스트라이핑은 일반적으로 ***사용 가능한 유일한 스트라이핑***이다.

<aside>
📌 요약하자면, 디스크 시스템에서 **병렬성의 두 가지 목적**이 있다.

1. 부하 균등화를 이용하여 **여러 작은 액세스(즉, 페이지 액세스)의 처리량을 높인다.**
2. 규모가 큰 액세스의 **응답 시간을 줄인다.**
</aside>

### 11.8.3 RAID 레벨

- 미러링은 높은 신뢰성을 제공하지만, 비용이 많이 든다.
- 스트라이핑은 높은 전송률을 제공하지만, 신뢰성을 향상할 수 없다.

⇒ 패리티 비트와 디스크 스트라이핑을 결합하여 ***적은 비용으로 중복을 허용하는 많은 기법이 제안되었다.*** → **RAID 레벨로 분류**된다.

![image](https://user-images.githubusercontent.com/87460638/236809709-4725fcec-424f-41e3-9a11-a21053409221.png)

**(a) RAID 레벨 0**

- 블록 레벨로 스트라이핑하는 드라이브 구성을 말한다.
- 미러링이나 패리티 비트 같은 어떤 중복 정보도 가지고 있지 않는 것을 말한다.

**(b) RAID 레벨 1**

- 드라이브 미러링을 사용한다.

**(c) RAID 레벨 4**

- **메모리-스타일 오류 수정 코드(ECC) 구성**이라고도 한다. ECC는 RAID5 및 6에서도 사용된다.

- ECC는 드라이브에 걸쳐서 블록을 스트라이핑하여 저장장치 배열에서 직접 사용될 수 있다.
    - ECC 블록이 하나만 있어도 RAID4는 실제로 오류를 수정할 수 있다.
    - 메모리 시스템과 달리 드라이브 컨트롤러는 섹터가 올바르게 읽혔는지 여부를 감지할 수 있으므로 단일 패리티 블록을 사용하여 오류를 수정하고 감지할 수 있다.
    
- 블록 읽기는 하나의 드라이브에서만 액세스하므로 다른 드라이브는 다른 요청을 처리할 수 있다.

- 모든 디스크를 병렬로 읽을 수 있으므로 대용량 읽기의 전송 속도가 높다.
    - 데이터와 패리티를 병렬로 쓸 수 있기 때문에 대용량 쓰기도 높은 전송 속도를 보인다.
    
- 작은 독립적인 쓰기는 병렬로 수행될 수 없다.
    - 운영체제가 블록보다 작은 데이터를 읽고, 쓰고 하려면 **읽기-수정-쓰기 주기** 로 수행된다.
    - 따라서 단일 쓰기에는 4번의 드라이브 액세스가 필요하다.

- 동일한 데이터 보호 기능을 제공함으로써 레벨1과 비교하면 2가지 장점이 있다.
    - 저장장치 오버 헤드가 줄어든다.
    - 블록 세트를 읽거나 쓰는 전송 속도는 레벨 1보다 N배 빠르다.

- 문제는 XOR 패리티 계산 및 기록 비용이다.
    - 오버헤드로 인해 패리티를 사용하지 않는 RAID 배열보다 쓰기 속도가 느려질 수 있다.

**(d) RAID 레벨 5**

- block-interleaved distributed parity라고 불리기도 한다.
    - 데이터와 패리티를 모든 N + 1 드라이브에 분산시킨다.
    - N개의 블록 집합에 대해 각 블록을 위해 하나의 드라이브가 패리티를 저장하고 다른 드라이브들이 데이터를 저장한다.

⇒ ***패리티를 모든 드라이브에 분산시킴***으로써 RAID 5는 ***RAID 4에서 발생 가능한 하나의 패리티 드라이브에 대한 과도한 집중을 막을 수 있다.***

**(e) RAID 레벨 6**

- **P+Q 중복 기법**이라고도 불린다.
    - 여러 디스크 오류에 대비하기 위해 추가의 중복 정보를 저장한다.
    - 패리티 비트를 사용하는 대신 **Galois field 수학**과 같은 에러 교정 코드가 Q를 계산하는 데 사용된다.
    - 이 기법은 **4블록마다 2블록의 중복 데이터를 저장한다.**
    - ⇒ 따라서 ***시스템은 2개의 드라이브 오류를 허용할 수 있다.***

**(f) 다차원 RAID 레벨 6**

- 드라이브를 행과 열(2차원 이상의 배열)로 논리적으로 정렬하고 행을 따라 수평으로 그리고 열을 따라 수직방향 아래로 RAID 레벨 6을 구현한다.
    - 시스템은 이러한 위치 중 하나에서 패리티 블록을 사용하여 모든 장애 또는 실제로 여러 장애로부터 복구할 수 있다.

**(g) RAID 레벨 0 + 1과 1+ 0**

> **RAID 레벨 0 + 1**
> 
- RAID 0 과 RAID 1을 조합한 것이다.
    - RAID 1이 신뢰성을 제공, RAID 0은 높은 성능을 제공한다.
    - 하지만 이것은 RAID 1처럼 드라이브의 수가 2배 필요하다.
    - ⇒ 따라서 구축 비용이 많이 소요된다.

> **RAID 레벨 1 + 0**
> 
- 드라이브는 쌍으로 미러링된 후 결과 미러링 된 쌍이 스트라이프 된다.
    - RAID 레벨 1 + 0에서 오류가 발생하면 단일 드라이브를 사용할 수 없지만,
    - 나머지 드라이브와 마찬가지로 드라이브를 미러링하는 드라이브는 계속 사용할 수 있다.

- 스냅숏이나 복제와 같은 특징들이 상기의 계층에 구현될 수 있다.
    - **스냅숏**은 마지막 갱신이 일어나기 전의 파일 시스템의 모습니다.
    - **복제**는 중복성과 복구를 위해 자동으로 분리된 지역에 같은 내용을 쓰는 것이다.

- 복제 방식은 비동기/동기 로 나뉜다.
    - 동기적 복제는 쓰기가 완료되었다고 판단되기 전에 각 블록은 로컬 영역과 원격 영역에 반드시 쓰여야 한다.
    - 비동기적 복제는 써야 할 내용을 묶음으로 만들어 주기적으로 쓰게 된다.

- 스냅숏이나 복제와 같은 것의 구현은 RAID가 구현된 계층에 의해 달라진다.
    - 만약 소프트웨어적으로 구현되었다면 → 각 호스트는 자신의 복제본을 만들고 관리해야 한다.
    - 만약 저장매체 배열이나 SAN에 구현되었다면 → 호스트의 운영체제 혹은 호스트의 특성과 관계없이 호스트들의 데이터는 복제될 수 있다.
    
- 대부분의 RAID 구현의 다른 측면 중 하나는 여분의 드라이브 또는 드라이브들이다.
    - **여분의 드라이브**는 데이터를 저장하지 않고, 다른 드라이브의 오류 시 대체하는 역할이다.

### 11.8.4 RAID 레벨 선택

> 다양한 선택이 가능하다면 설계자는 어떤 RAID 레벨을 선택해야 하는가 ?
> 
> - 고려사항 중 하나는 **복구 능력**이다.

- 복구는 다른 드라이브에 모든 정보를 복사하는 RAID 1이 가장 쉽다.
    - 다른 레벨에서는 오류 드라이브의 정보를 복구하기 위해 다른 모든 드라이브에 접근해야 한다.

> 몇 개의 드라이브로 RAID를 구성할 것인가 ? 
몇 비트가 각 패리티 비트로 보호되어야 할 것인가 ?
> 
> - 드라이브가 추가된다면 → 데이터 전송률은 높지만 시스템 구축 비용은 많이 소요된다.
> - 많은 비트가 패리티 비트로 보호된다면 → 패리티 비으틔 오버헤드로 인한 드라이브 공간은 줄어들지만 데이터 분실을 야기할 수 있다.

### 11.8.5 확장

- 테이프의 경우
    - 여러 테이프 중 하나가 손상을 입어도 RAID 구조는 데이터를 복사할 수 있다.

- 데이터 브로드캐스팅의 경우
    - 데이터의 블록은 작은 유닛으로 나누어지고 패리티 유닛과 함께 브로드캐스팅된다.

- 일반적으로 여러 테이프 드라이브를 가지는 테이프 드라이브 로봇은 처리량을 늘리고 백업 시간을 줄이기 위해 모든 드라이브에 걸쳐 데이터를 스트라이핑 한다.

### 11.8.6 RAID의 문제점들

- RAID는 운영체제나 사용자를 위해 데이터의 사용을 항상 보장하지 않는다.

- **Solaris ZFS** 파일 시스템은 체크섬을 사용하여 문제를 해결하는 방식을 취한다.
    - ZFS는 모든 블록의 내부적인 체크섬을 유지한다.
    - 이때 블록을 가리키는 포인터와 함께 저장된다.
    
    ![image](https://user-images.githubusercontent.com/87460638/236809804-0425e0cd-f4a8-4710-bbd5-5bd61f8e36fa.png)
    

- ZFS의 성능이 매우 빠르기 때문에 체크섬 계산이나 읽기-변경-쓰기 주기가 크지 않으므로 추가적인 오버헤드는 두드러지지 않는다.

- ZFS는 파일 시스템 관리와 볼륨 관리를 하나의 단위로 묶음으로써 전통적인 분리 방식이 허용하는 것에 비해 더 많은 기능을 제공한다.
    - 드라이브 또는 드라이브의 파티션은 RAID 집합을 통해 집합적으로 저장장치의 **풀**을 구성한다.
    - 이 풀은 하나 또는 그 이상의 ZFS 파일 시스템을 저장할 수 있다.

- ZFS는 파일 시스템의 크기를 제한하기 위해 **쿼터**를 제공하고, 파일 시스템이 지정된 크기만큼의 확장을 보장하기 위해 **예약**을 제공한다.

### 11.8.7 객체 저장소

- 객체 지향은 사용자 지향적이 아니라 **컴퓨터 지향적**이다,
    - 따라서 프로그램에서 사용되도록 설계되었고, 일반적인 순서는 다음과 같다.
    1. 저장장치 풀 내에 객체를 생성하고 **객체 ID를 받는다.**
    2. 필요할 대 객체 ID를 통해 **객체에 접근**한다.
    3. 객체 ID를 통해 **객체를 삭제**한다.

- **HDFS(Hadoop File System)** 및 **Ceph**와 같은 객체 저장소 관리 소프트웨어는 객체를 저장할 위치를 결정하고 객체 보호를 관리한다.
    - 일반적으로 이는 상용 하드웨어에서 발생한다.
    
- 객체 저장소는 **수평 확장성**의 이점이 있다.
    - 즉, 저장장치 배열에는 고정된 최대 용량이 있지만 객체 용량을 추가하기 위해 내부 디스크 또는 연결된 외부 디스크가 있는 컴퓨터를 더 추가하고 저장장치를 풀에 추가하기만 하면 된다.
- 또다른 특징은 각 객체가 내용에 대한 설명을 포함하고 있기 때문에 자체 설명하고 있다는 점이다.
    - 실제로 객체 저장소는 콘텐츠를 기반으로 검색할 수 있기 때문에 콘텐츠 **주소지정가능 저장소** 라고도 한다.
    - 콘텐츠의 형식은 설정되어 있지 않으므로 시스템이 저장하는 것은 **구조화되지 않은 데이터**이다.
